{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ashwinramesh/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "import socket\n",
    "import json\n",
    "import nltk\n",
    "import operator\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stack:\n",
    "    def __init__(self):\n",
    "        self.__storage = []\n",
    "\n",
    "    def isEmpty(self):\n",
    "        return len(self.__storage) == 0\n",
    "\n",
    "    def push(self,p):\n",
    "        self.__storage.append(p)\n",
    "\n",
    "    def pop(self):\n",
    "        return self.__storage.pop()\n",
    "    \n",
    "    def pop_val1(self):\n",
    "        return self.__storage.pop()[0]\n",
    "    \n",
    "    def pop_val2(self):\n",
    "        return self.__storage.pop()[1]\n",
    "    \n",
    "    def top(self):\n",
    "        return self.__storage[-1]\n",
    "    \n",
    "    def top_at_pos_from_top(self, pos):\n",
    "        return self.__storage[self.size() - pos - 1]\n",
    "    \n",
    "    def top_val1(self):\n",
    "        return self.__storage[-1][0]\n",
    "    \n",
    "    def top_val1_at_pos_from_top(self, pos):\n",
    "        return self.__storage[self.size() - pos - 1][0]\n",
    "    \n",
    "    def top_val2(self):\n",
    "        return self.__storage[-1][1]\n",
    "    \n",
    "    def top_val2_at_pos_from_top(self, pos):\n",
    "        return self.__storage[self.size() - pos - 1][1]\n",
    "    \n",
    "    def combine_val1(self, cur_val):\n",
    "        top_val = self.pop()\n",
    "        self.push((top_val[0] + ' ' + cur_val, top_val[1]))\n",
    "\n",
    "    def update_val2(self, cur_val):\n",
    "        top_val = self.pop()\n",
    "        self.push((top_val[0], cur_val))\n",
    "        \n",
    "    def update_val2_at_pos_from_top(self, cur_val, pos):\n",
    "        self.__storage[self.size() - pos - 1] = (self.__storage[self.size() - pos - 1][0],cur_val) # Tuples are immutable\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.__storage)\n",
    "    \n",
    "    def prnt(self):\n",
    "        if len(self.__storage) > 0:\n",
    "            return str(self.__storage)\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "global _nlp\n",
    "global _ip\n",
    "global _port\n",
    "global _buffer_size\n",
    "global _sen_analyzer\n",
    "\n",
    "_nlp = None\n",
    "_ip = '100.81.36.227'\n",
    "_port = 7183\n",
    "_buffer_size = 1024\n",
    "_sen_analyzer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    global _nlp\n",
    "    global _sen_analyzer\n",
    "    _nlp = StanfordCoreNLP(r'stanford-corenlp-full-2017-06-09')\n",
    "    _sen_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    _nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def send_over_socket(data):\n",
    "    if len(data) == 0:\n",
    "        return\n",
    "    print \"Send Data: \" + str(data)\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client_socket.connect((_ip, _port))\n",
    "    client_socket.send(json.dumps(data))\n",
    "    print \"Sent Data\"\n",
    "    client_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    #d = {'title': 'Trump’s Promises to Kim Jong-un Leave U.S. and Allies Scrambling', 'content': 'North Korea’s extravagant coverage of the meeting, said Joseph Y. Yun, a former State Department official who negotiated with North Korea, suggested that Mr. Kim might want a different relationship with the United States.'}\n",
    "    #df = pd.DataFrame(data=d, index=[0])\n",
    "    #return df\n",
    "    file_path = 'data/news.csv'\n",
    "    articles = pd.read_csv(file_path) #CoreNLP\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_named_entity(phrase):\n",
    "    entity_dict = {}\n",
    "    allowed_entities = ['PERSON', 'ORGANIZATION', 'LOCATION']\n",
    "    for pair in _nlp.ner(phrase):\n",
    "        if pair[1] in allowed_entities:\n",
    "            entity_dict[allowed_entities.index(pair[1])] = pair[1]\n",
    "    if len(entity_dict) == 0:\n",
    "        return \"-\"\n",
    "    else:\n",
    "        entity_dict_sorted = sorted(entity_dict.items())\n",
    "        return entity_dict_sorted[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment(phrase):\n",
    "    sentiment_dict = {'pos': '2', 'neg': '-2', 'neu': '1'}\n",
    "    sentiment = _sen_analyzer.polarity_scores(phrase)\n",
    "    del sentiment['compound']\n",
    "    max_key = max(sentiment.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    if sentiment[max_key] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sentiment_dict[max_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# noun_st: stack to hold all the nouns; val1 refers to noun and val2 refers to the level of nesting\n",
    "# verb_st: stack to hold all the verbs; val1 refers to verb and val2 refers to the level of nesting\n",
    "# prev_added_verb: helps determine if the verbs are consecutive and can be combined\n",
    "# cur_nest: gives the level of nesting of the tags on the current line ( got using the number of tabs at the beginning of the line)\n",
    "# nouns_popped_count: Add two nouns at a time to noun list\n",
    "\n",
    "def parse_dependency_tree(tree):\n",
    "    # Define patterns for Regular expression matching\n",
    "    open_b, close_b = '()'\n",
    "    open_pattern, close_pattern = (re.escape(open_b), re.escape(close_b))\n",
    "    node_pattern = '[^\\s%s%s]+' % (open_pattern, close_pattern)\n",
    "    leaf_pattern = '[^\\s%s%s]+' % (open_pattern, close_pattern)\n",
    "    token_re = re.compile('%s\\s*(%s)?|%s|(%s)' % (\n",
    "            open_pattern, node_pattern, close_pattern, leaf_pattern))\n",
    "    \n",
    "    # Define other variables in the function whose values are constant or needs to be retained between lines of the tree\n",
    "    prev_added_verb = False\n",
    "    allowed_NP = ['NP', 'NN', 'NNP', 'NNPS', 'NNS', 'JJ', 'CD']\n",
    "    allowed_VP = ['VP', 'VB', 'VBP', 'VBZ', 'VBD', 'VBG', 'VBN', 'MD', 'TO', 'PP']\n",
    "    prev_nest,cur_nest = 0,0\n",
    "    noun_st = Stack()\n",
    "    verb_st = Stack()    \n",
    "    noun_list = []\n",
    "    verb_list = []\n",
    "    ner_list = []\n",
    "    sen_list = []\n",
    "    \n",
    "    # For every line in the tree\n",
    "    for line in tree.split('\\n'):\n",
    "        print '\\nLINE: ' + line\n",
    "        \n",
    "        #### 1. Find and process all matching patterns ####\n",
    "        \n",
    "        main_tag, cur_tag, noun, verb = '','','',''\n",
    "        prev_nest = cur_nest\n",
    "        cur_nest = len(line) - len(line.lstrip(' '))\n",
    "        \n",
    "        for match in token_re.finditer(line):\n",
    "            token = match.group()\n",
    "            # Beginning of a tree/subtree\n",
    "            if token[0] == open_b:\n",
    "                if main_tag == '':\n",
    "                    # main_tag is usually ROOT, S, NP, VP, JJ etc\n",
    "                    main_tag = token[1:].lstrip()\n",
    "                else:\n",
    "                    # get the current tag, the nested one\n",
    "                    cur_tag = token[1:].lstrip()    \n",
    "            # End of a tree/subtree - nothing to do\n",
    "            elif token == close_b:\n",
    "                ignore = 1\n",
    "            # Leaf node\n",
    "            else:\n",
    "                # if main tag is a noun (or adjective) and the nested tags are part of the noun, combine them all\n",
    "                if main_tag in allowed_NP and (cur_tag in allowed_NP or cur_tag == ''):\n",
    "                    noun += token + ' '\n",
    "                # if main tag is a verb (or preposition) and the nested tags are part of the verb, combine them all\n",
    "                elif main_tag in allowed_VP and (cur_tag in allowed_VP or cur_tag == ''):\n",
    "                    verb += token + ' '\n",
    "                # if main tag is a preposition, regardless of inner tags, consider it a verb\n",
    "                elif main_tag == 'PP':\n",
    "                    verb += token + ' '\n",
    "\n",
    "        print '#### 1. Find and process all matching patterns ####'\n",
    "        print 'NOUN: ' + noun.rstrip()\n",
    "        print 'VERB: ' + verb.rstrip()\n",
    "        print 'MAIN_TAG: ' + main_tag\n",
    "        print 'CUR_TAG: ' + cur_tag\n",
    "        print 'PREV NEST: ' + str(prev_nest)\n",
    "        print 'CUR NEST: ' + str(cur_nest)\n",
    "        \n",
    "        #### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
    "        \n",
    "        if cur_nest < prev_nest:\n",
    "            print '#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####'\n",
    "            while noun_st.size() > 0 and cur_nest <= noun_st.top_val2():\n",
    "                if noun_st.size() > 1:\n",
    "                    popped_noun = noun_st.pop()\n",
    "                    if cur_nest <= noun_st.top_val2():\n",
    "                        v1 = popped_noun[0];\n",
    "                        v2 = noun_st.top()[0];\n",
    "                        noun_list.append(v1)\n",
    "                        noun_list.append(v2)\n",
    "                        ner_list.append(get_named_entity(v1))\n",
    "                        ner_list.append(get_named_entity(v2))\n",
    "                    else:\n",
    "                        if cur_nest <= verb_st.top_val2():\n",
    "                            v1 = popped_noun[0];\n",
    "                            v2 = noun_st.top()[0];\n",
    "                            noun_list.append(v1)\n",
    "                            noun_list.append(v2)\n",
    "                            ner_list.append(get_named_entity(v1))\n",
    "                            ner_list.append(get_named_entity(v2))\n",
    "                        else:\n",
    "                            noun_st.push(popped_noun)\n",
    "                            break\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "                if verb_st.size() > 0 and (verb_st.top_val2() >= noun_st.top_val2() or verb_st.top_val2() >= popped_noun[1]):\n",
    "                    e = verb_st.pop_val1()\n",
    "                    verb_list.append(e)\n",
    "                    sen_list.append(get_sentiment(e))\n",
    "                else:\n",
    "                    verb_list.append('-') \n",
    "                    sen_list.append(1)\n",
    "                    \n",
    "            if noun_st.size() > 0:\n",
    "                noun_st.update_val2(min(cur_nest,noun_st.top_val2()))\n",
    "                    \n",
    "            cur_pos_from_top = 0\n",
    "            while verb_st.size() > cur_pos_from_top and cur_nest < verb_st.top_val2_at_pos_from_top(cur_pos_from_top):\n",
    "                verb_st.update_val2_at_pos_from_top(cur_nest, cur_pos_from_top)\n",
    "                cur_pos_from_top += 1\n",
    "\n",
    "            print 'NOUN STACK: ' + noun_st.prnt()\n",
    "            print 'VERB STACK: ' + verb_st.prnt()\n",
    "            print 'NOUN LIST: ' + str(noun_list)\n",
    "            print 'VERB LIST: ' + str(verb_list)\n",
    "            print 'ENTITY LIST: ' + str(ner_list)\n",
    "            print 'SENTIMENT LIST: ' + str(sen_list)\n",
    "                \n",
    "        #### 3. Add necessary elements to stack ####\n",
    "    \n",
    "        if noun != '':\n",
    "            noun_st.push((noun.rstrip(), cur_nest))\n",
    "            prev_added_verb = False\n",
    "            \n",
    "        if verb != '':\n",
    "            if prev_added_verb == True and verb_st.size() > 0:\n",
    "                verb_st.combine_val1(verb.rstrip())\n",
    "            else:\n",
    "                verb_st.push((verb.rstrip(), cur_nest))\n",
    "            prev_added_verb = True\n",
    "\n",
    "        print '#### 3. Add necessary elements to stack ####'\n",
    "        print 'NOUN STACK: ' + noun_st.prnt()\n",
    "        print 'VERB STACK: ' + verb_st.prnt()\n",
    "        \n",
    "    return noun_list, verb_list, ner_list, sen_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_data(data):\n",
    "    # For each article\n",
    "    for index, row in data.iterrows(): \n",
    "        # Get the content\n",
    "        content = row['content']\n",
    "        \n",
    "        # Split content into sentenes\n",
    "        result = _nlp.annotate(content,\n",
    "                               properties={\n",
    "                                   'annotators': 'ssplit',\n",
    "                                   'outputFormat': 'json'\n",
    "                               })\n",
    "        annotated_content = json.loads(result)\n",
    "\n",
    "        # For each sentence\n",
    "        for annotated_sentence in annotated_content['sentences']:\n",
    "            sentence = ' '.join([t['word'] for t in annotated_sentence['tokens']])\n",
    "            \n",
    "            # Get the dependency tree for the sentence\n",
    "            tree = _nlp.parse(sentence)\n",
    "            vertices, edges, entities, sentiment = parse_dependency_tree(tree)\n",
    "            print '\\n\\nSEND OVER SOCKET'\n",
    "            socket_data = {}\n",
    "            socket_data['operation'] = 'append_graph_data'\n",
    "            socket_data['vertices'] = vertices\n",
    "            socket_data['edges'] = edges\n",
    "            socket_data['entities'] = entities\n",
    "            socket_data['sentiment'] = sentiment\n",
    "            send_over_socket(socket_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample_results():\n",
    "    socket_data = {}\n",
    "    socket_data['operation'] = 'get_all_persons'\n",
    "    #socket_data['operation'] = 'get_all_persons_with_degree'\n",
    "    #socket_data['operation'] = 'get_sentiment_around_person'\n",
    "    #socket_data['person'] = 'Trump'\n",
    "    send_over_socket(socket_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    init()\n",
    "    data = get_data()\n",
    "    parse_data(data)\n",
    "    #get_sample_results()\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LINE: (ROOT\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: ROOT\n",
      "CUR_TAG: \n",
      "PREV NEST: 0\n",
      "CUR NEST: 0\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: \n",
      "\n",
      "LINE:   (S\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: S\n",
      "CUR_TAG: \n",
      "PREV NEST: 0\n",
      "CUR NEST: 2\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: \n",
      "\n",
      "LINE:     (NP (NNP President) (NNP Trump))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: President Trump\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNP\n",
      "PREV NEST: 2\n",
      "CUR NEST: 4\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4)]\n",
      "VERB STACK: \n",
      "\n",
      "LINE:     (VP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: \n",
      "PREV NEST: 4\n",
      "CUR NEST: 4\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4)]\n",
      "VERB STACK: \n",
      "\n",
      "LINE:       (VP (VBD praised)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: praised\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VBD\n",
      "PREV NEST: 4\n",
      "CUR NEST: 6\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4)]\n",
      "VERB STACK: [(u'praised', 6)]\n",
      "\n",
      "LINE:         (NP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: \n",
      "PREV NEST: 6\n",
      "CUR NEST: 8\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4)]\n",
      "VERB STACK: [(u'praised', 6)]\n",
      "\n",
      "LINE:           (NP (DT the) (NN strongman) (NNS tactics))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: strongman tactics\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNS\n",
      "PREV NEST: 8\n",
      "CUR NEST: 10\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'strongman tactics', 10)]\n",
      "VERB STACK: [(u'praised', 6)]\n",
      "\n",
      "LINE:           (PP (IN of)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: of\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 10\n",
      "CUR NEST: 10\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'strongman tactics', 10)]\n",
      "VERB STACK: [(u'praised', 6), (u'of', 10)]\n",
      "\n",
      "LINE:             (NP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: \n",
      "PREV NEST: 10\n",
      "CUR NEST: 12\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'strongman tactics', 10)]\n",
      "VERB STACK: [(u'praised', 6), (u'of', 10)]\n",
      "\n",
      "LINE:               (NP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: \n",
      "PREV NEST: 12\n",
      "CUR NEST: 14\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'strongman tactics', 10)]\n",
      "VERB STACK: [(u'praised', 6), (u'of', 10)]\n",
      "\n",
      "LINE:                 (NP (NNP North) (NNP Korea) (POS 's))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: North Korea\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: POS\n",
      "PREV NEST: 14\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'strongman tactics', 10), (u'North Korea', 16)]\n",
      "VERB STACK: [(u'praised', 6), (u'of', 10)]\n",
      "\n",
      "LINE:                 (NN leader))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: leader\n",
      "VERB: \n",
      "MAIN_TAG: NN\n",
      "CUR_TAG: \n",
      "PREV NEST: 16\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'strongman tactics', 10), (u'North Korea', 16), (u'leader', 16)]\n",
      "VERB STACK: [(u'praised', 6), (u'of', 10)]\n",
      "\n",
      "LINE:               (, ,)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: ,\n",
      "CUR_TAG: \n",
      "PREV NEST: 16\n",
      "CUR NEST: 14\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'strongman tactics', 10), (u'North Korea', 14)]\n",
      "VERB STACK: [(u'praised', 6), (u'of', 10)]\n",
      "NOUN LIST: [u'leader', u'North Korea']\n",
      "VERB LIST: ['-']\n",
      "ENTITY LIST: ['-', u'LOCATION']\n",
      "SENTIMENT LIST: [1]\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'strongman tactics', 10), (u'North Korea', 14)]\n",
      "VERB STACK: [(u'praised', 6), (u'of', 10)]\n",
      "\n",
      "LINE:               (NP (NNP Kim) (NNP Jong-un))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: Kim Jong-un\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNP\n",
      "PREV NEST: 14\n",
      "CUR NEST: 14\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'strongman tactics', 10), (u'North Korea', 14), (u'Kim Jong-un', 14)]\n",
      "VERB STACK: [(u'praised', 6), (u'of', 10)]\n",
      "\n",
      "LINE:               (, ,)))))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: ,\n",
      "CUR_TAG: \n",
      "PREV NEST: 14\n",
      "CUR NEST: 14\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'strongman tactics', 10), (u'North Korea', 14), (u'Kim Jong-un', 14)]\n",
      "VERB STACK: [(u'praised', 6), (u'of', 10)]\n",
      "\n",
      "LINE:       (CC and)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: CC\n",
      "CUR_TAG: \n",
      "PREV NEST: 14\n",
      "CUR NEST: 6\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
      "NOUN STACK: [(u'President Trump', 4)]\n",
      "VERB STACK: \n",
      "NOUN LIST: [u'leader', u'North Korea', u'Kim Jong-un', u'North Korea', u'North Korea', u'strongman tactics', u'strongman tactics', u'President Trump']\n",
      "VERB LIST: ['-', '-', u'of', u'praised']\n",
      "ENTITY LIST: ['-', u'LOCATION', u'PERSON', u'LOCATION', u'LOCATION', '-', '-', u'PERSON']\n",
      "SENTIMENT LIST: [1, 1, '1', '2']\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4)]\n",
      "VERB STACK: \n",
      "\n",
      "LINE:       (VP (VBD dismissed)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: dismissed\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VBD\n",
      "PREV NEST: 6\n",
      "CUR NEST: 6\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4)]\n",
      "VERB STACK: [(u'dismissed', 6)]\n",
      "\n",
      "LINE:         (NP (JJ American) (JJ military) (NNS exercises))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: American military exercises\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNS\n",
      "PREV NEST: 6\n",
      "CUR NEST: 8\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'American military exercises', 8)]\n",
      "VERB STACK: [(u'dismissed', 6)]\n",
      "\n",
      "LINE:         (PP (IN with)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: with\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 8\n",
      "CUR NEST: 8\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'American military exercises', 8)]\n",
      "VERB STACK: [(u'dismissed', 6), (u'with', 8)]\n",
      "\n",
      "LINE:           (NP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: \n",
      "PREV NEST: 8\n",
      "CUR NEST: 10\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'American military exercises', 8)]\n",
      "VERB STACK: [(u'dismissed', 6), (u'with', 8)]\n",
      "\n",
      "LINE:             (NP (NNP South) (NNP Korea))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: South Korea\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNP\n",
      "PREV NEST: 10\n",
      "CUR NEST: 12\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'American military exercises', 8), (u'South Korea', 12)]\n",
      "VERB STACK: [(u'dismissed', 6), (u'with', 8)]\n",
      "\n",
      "LINE:             (PP (IN as)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: as\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 12\n",
      "CUR NEST: 12\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'American military exercises', 8), (u'South Korea', 12)]\n",
      "VERB STACK: [(u'dismissed', 6), (u'with', 8), (u'as', 12)]\n",
      "\n",
      "LINE:               (NP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: \n",
      "PREV NEST: 12\n",
      "CUR NEST: 14\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'American military exercises', 8), (u'South Korea', 12)]\n",
      "VERB STACK: [(u'dismissed', 6), (u'with', 8), (u'as', 12)]\n",
      "\n",
      "LINE:                 (NP (DT a) (NN waste))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: waste\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NN\n",
      "PREV NEST: 14\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'American military exercises', 8), (u'South Korea', 12), (u'waste', 16)]\n",
      "VERB STACK: [(u'dismissed', 6), (u'with', 8), (u'as', 12)]\n",
      "\n",
      "LINE:                 (PP (IN of)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: of\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 16\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'American military exercises', 8), (u'South Korea', 12), (u'waste', 16)]\n",
      "VERB STACK: [(u'dismissed', 6), (u'with', 8), (u'as', 12), (u'of', 16)]\n",
      "\n",
      "LINE:                   (NP (NN money)))))))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: money\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NN\n",
      "PREV NEST: 16\n",
      "CUR NEST: 18\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'American military exercises', 8), (u'South Korea', 12), (u'waste', 16), (u'money', 18)]\n",
      "VERB STACK: [(u'dismissed', 6), (u'with', 8), (u'as', 12), (u'of', 16)]\n",
      "\n",
      "LINE:         (PP (IN on)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: on\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 18\n",
      "CUR NEST: 8\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN STACK: [(u'President Trump', 4), (u'American military exercises', 8)]\n",
      "VERB STACK: [(u'dismissed', 6)]\n",
      "NOUN LIST: [u'leader', u'North Korea', u'Kim Jong-un', u'North Korea', u'North Korea', u'strongman tactics', u'strongman tactics', u'President Trump', u'money', u'waste', u'waste', u'South Korea', u'South Korea', u'American military exercises']\n",
      "VERB LIST: ['-', '-', u'of', u'praised', u'of', u'as', u'with']\n",
      "ENTITY LIST: ['-', u'LOCATION', u'PERSON', u'LOCATION', u'LOCATION', '-', '-', u'PERSON', '-', '-', '-', u'LOCATION', u'LOCATION', '-']\n",
      "SENTIMENT LIST: [1, 1, '1', '2', '1', '1', '1']\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'American military exercises', 8)]\n",
      "VERB STACK: [(u'dismissed', 6), (u'on', 8)]\n",
      "\n",
      "LINE:           (NP (NNP Friday)))))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: Friday\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNP\n",
      "PREV NEST: 8\n",
      "CUR NEST: 10\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4), (u'American military exercises', 8), (u'Friday', 10)]\n",
      "VERB STACK: [(u'dismissed', 6), (u'on', 8)]\n",
      "\n",
      "LINE:     (. .)))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: .\n",
      "CUR_TAG: \n",
      "PREV NEST: 10\n",
      "CUR NEST: 4\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
      "NOUN STACK: [(u'President Trump', 4)]\n",
      "VERB STACK: \n",
      "NOUN LIST: [u'leader', u'North Korea', u'Kim Jong-un', u'North Korea', u'North Korea', u'strongman tactics', u'strongman tactics', u'President Trump', u'money', u'waste', u'waste', u'South Korea', u'South Korea', u'American military exercises', u'Friday', u'American military exercises', u'American military exercises', u'President Trump']\n",
      "VERB LIST: ['-', '-', u'of', u'praised', u'of', u'as', u'with', u'on', u'dismissed']\n",
      "ENTITY LIST: ['-', u'LOCATION', u'PERSON', u'LOCATION', u'LOCATION', '-', '-', u'PERSON', '-', '-', '-', u'LOCATION', u'LOCATION', '-', '-', '-', '-', u'PERSON']\n",
      "SENTIMENT LIST: [1, 1, '1', '2', '1', '1', '1', '1', '1']\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'President Trump', 4)]\n",
      "VERB STACK: \n",
      "\n",
      "\n",
      "SEND OVER SOCKET\n",
      "Send Data: {'entities': ['-', u'LOCATION', u'PERSON', u'LOCATION', u'LOCATION', '-', '-', u'PERSON', '-', '-', '-', u'LOCATION', u'LOCATION', '-', '-', '-', '-', u'PERSON'], 'operation': 'append_graph_data', 'edges': ['-', '-', u'of', u'praised', u'of', u'as', u'with', u'on', u'dismissed'], 'vertices': [u'leader', u'North Korea', u'Kim Jong-un', u'North Korea', u'North Korea', u'strongman tactics', u'strongman tactics', u'President Trump', u'money', u'waste', u'waste', u'South Korea', u'South Korea', u'American military exercises', u'Friday', u'American military exercises', u'American military exercises', u'President Trump'], 'sentiment': [1, 1, '1', '2', '1', '1', '1', '1', '1']}\n",
      "Sent Data\n",
      "\n",
      "LINE: (ROOT\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: ROOT\n",
      "CUR_TAG: \n",
      "PREV NEST: 0\n",
      "CUR NEST: 0\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: \n",
      "\n",
      "LINE:   (S\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: S\n",
      "CUR_TAG: \n",
      "PREV NEST: 0\n",
      "CUR NEST: 2\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: \n",
      "\n",
      "LINE:     (PP (IN Along)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: Along\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 2\n",
      "CUR NEST: 4\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: [(u'Along', 4)]\n",
      "\n",
      "LINE:       (PP (IN with)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: with\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 4\n",
      "CUR NEST: 6\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: [(u'Along with', 4)]\n",
      "\n",
      "LINE:         (NP (CD 10) (JJ other) (NNS nations))))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: 10 other nations\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNS\n",
      "PREV NEST: 6\n",
      "CUR NEST: 8\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 8)]\n",
      "VERB STACK: [(u'Along with', 4)]\n",
      "\n",
      "LINE:     (, ,)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: ,\n",
      "CUR_TAG: \n",
      "PREV NEST: 8\n",
      "CUR NEST: 4\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
      "NOUN STACK: [(u'10 other nations', 4)]\n",
      "VERB STACK: [(u'Along with', 4)]\n",
      "NOUN LIST: []\n",
      "VERB LIST: []\n",
      "ENTITY LIST: []\n",
      "SENTIMENT LIST: []\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4)]\n",
      "VERB STACK: [(u'Along with', 4)]\n",
      "\n",
      "LINE:     (NP (NNP Canada))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: Canada\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNP\n",
      "PREV NEST: 4\n",
      "CUR NEST: 4\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4)]\n",
      "\n",
      "LINE:     (VP (VBZ is)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: is\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VBZ\n",
      "PREV NEST: 4\n",
      "CUR NEST: 4\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is', 4)]\n",
      "\n",
      "LINE:       (VP (VBG trying)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: trying\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VBG\n",
      "PREV NEST: 4\n",
      "CUR NEST: 6\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying', 4)]\n",
      "\n",
      "LINE:         (S\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: S\n",
      "CUR_TAG: \n",
      "PREV NEST: 6\n",
      "CUR NEST: 8\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying', 4)]\n",
      "\n",
      "LINE:           (VP (TO to)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: to\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: TO\n",
      "PREV NEST: 8\n",
      "CUR NEST: 10\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to', 4)]\n",
      "\n",
      "LINE:             (VP (VB revive)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: revive\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VB\n",
      "PREV NEST: 10\n",
      "CUR NEST: 12\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:               (NP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: \n",
      "PREV NEST: 12\n",
      "CUR NEST: 14\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                 (NP (DT the) (NNP Trans-Pacific) (NNP Partnership))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: Trans-Pacific Partnership\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNP\n",
      "PREV NEST: 14\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                 (, ,)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: ,\n",
      "CUR_TAG: \n",
      "PREV NEST: 16\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                 (NP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: \n",
      "PREV NEST: 16\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                   (NP (DT a) (NN trade) (NN deal))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: trade deal\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NN\n",
      "PREV NEST: 16\n",
      "CUR NEST: 18\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                   (VP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: \n",
      "PREV NEST: 18\n",
      "CUR NEST: 18\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                     (VP (VBN championed)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: championed\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VBN\n",
      "PREV NEST: 18\n",
      "CUR NEST: 20\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4), (u'championed', 20)]\n",
      "\n",
      "LINE:                       (PP (IN by)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: by\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 20\n",
      "CUR NEST: 22\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4), (u'championed by', 20)]\n",
      "\n",
      "LINE:                         (NP (DT the) (NNP Obama) (NN administration))))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: Obama administration\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NN\n",
      "PREV NEST: 22\n",
      "CUR NEST: 24\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18), (u'Obama administration', 24)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4), (u'championed by', 20)]\n",
      "\n",
      "LINE:                     (CC and)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: CC\n",
      "CUR_TAG: \n",
      "PREV NEST: 24\n",
      "CUR NEST: 20\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "NOUN LIST: [u'Obama administration', u'trade deal']\n",
      "VERB LIST: [u'championed by']\n",
      "ENTITY LIST: [u'PERSON', '-']\n",
      "SENTIMENT LIST: ['2']\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                     (VP (VBN abandoned)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: abandoned\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VBN\n",
      "PREV NEST: 20\n",
      "CUR NEST: 20\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4), (u'abandoned', 20)]\n",
      "\n",
      "LINE:                       (PP (IN by)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: by\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 20\n",
      "CUR NEST: 22\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4), (u'abandoned by', 20)]\n",
      "\n",
      "LINE:                         (NP (NNP Mr.) (NNP Trump))))))))))))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: Mr. Trump\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNP\n",
      "PREV NEST: 22\n",
      "CUR NEST: 24\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18), (u'Mr. Trump', 24)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4), (u'abandoned by', 20)]\n",
      "\n",
      "LINE:     (. .)))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: .\n",
      "CUR_TAG: \n",
      "PREV NEST: 24\n",
      "CUR NEST: 4\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
      "NOUN STACK: [(u'10 other nations', 4)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB STACK: \n",
      "NOUN LIST: [u'Obama administration', u'trade deal', u'Mr. Trump', u'trade deal', u'trade deal', u'Trans-Pacific Partnership', u'Trans-Pacific Partnership', u'Canada', u'Canada', u'10 other nations']\n",
      "VERB LIST: [u'championed by', u'abandoned by', '-', u'is trying to revive', u'Along with']\n",
      "ENTITY LIST: [u'PERSON', '-', u'PERSON', '-', '-', '-', '-', u'LOCATION', u'LOCATION', '-']\n",
      "SENTIMENT LIST: ['2', '-2', 1, '1', '1']\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4)]\n",
      "VERB STACK: \n",
      "\n",
      "\n",
      "SEND OVER SOCKET\n",
      "Send Data: {'entities': [u'PERSON', '-', u'PERSON', '-', '-', '-', '-', u'LOCATION', u'LOCATION', '-'], 'operation': 'append_graph_data', 'edges': [u'championed by', u'abandoned by', '-', u'is trying to revive', u'Along with'], 'vertices': [u'Obama administration', u'trade deal', u'Mr. Trump', u'trade deal', u'trade deal', u'Trans-Pacific Partnership', u'Trans-Pacific Partnership', u'Canada', u'Canada', u'10 other nations'], 'sentiment': ['2', '-2', 1, '1', '1']}\n",
      "Sent Data\n",
      "\n",
      "LINE: (ROOT\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: ROOT\n",
      "CUR_TAG: \n",
      "PREV NEST: 0\n",
      "CUR NEST: 0\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: \n",
      "\n",
      "LINE:   (S\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: S\n",
      "CUR_TAG: \n",
      "PREV NEST: 0\n",
      "CUR NEST: 2\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: \n",
      "\n",
      "LINE:     (NP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: \n",
      "PREV NEST: 2\n",
      "CUR NEST: 4\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: \n",
      "\n",
      "LINE:       (NP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: \n",
      "PREV NEST: 4\n",
      "CUR NEST: 6\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: \n",
      "\n",
      "LINE:         (NP (NNP North) (NNP Korea) (POS 's))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: North Korea\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: POS\n",
      "PREV NEST: 6\n",
      "CUR NEST: 8\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 8)]\n",
      "VERB STACK: \n",
      "\n",
      "LINE:         (JJ extravagant) (NN coverage))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: extravagant coverage\n",
      "VERB: \n",
      "MAIN_TAG: JJ\n",
      "CUR_TAG: NN\n",
      "PREV NEST: 8\n",
      "CUR NEST: 8\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 8), (u'extravagant coverage', 8)]\n",
      "VERB STACK: \n",
      "\n",
      "LINE:       (PP (IN of)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: of\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 8\n",
      "CUR NEST: 6\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
      "NOUN STACK: [(u'North Korea', 6)]\n",
      "VERB STACK: \n",
      "NOUN LIST: [u'extravagant coverage', u'North Korea']\n",
      "VERB LIST: ['-']\n",
      "ENTITY LIST: ['-', u'LOCATION']\n",
      "SENTIMENT LIST: [1]\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 6)]\n",
      "VERB STACK: [(u'of', 6)]\n",
      "\n",
      "LINE:         (NP (DT the) (NN meeting))))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: meeting\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NN\n",
      "PREV NEST: 6\n",
      "CUR NEST: 8\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 6), (u'meeting', 8)]\n",
      "VERB STACK: [(u'of', 6)]\n",
      "\n",
      "LINE:     (, ,)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: ,\n",
      "CUR_TAG: \n",
      "PREV NEST: 8\n",
      "CUR NEST: 4\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
      "NOUN STACK: [(u'North Korea', 4)]\n",
      "VERB STACK: \n",
      "NOUN LIST: [u'extravagant coverage', u'North Korea', u'meeting', u'North Korea']\n",
      "VERB LIST: ['-', u'of']\n",
      "ENTITY LIST: ['-', u'LOCATION', '-', u'LOCATION']\n",
      "SENTIMENT LIST: [1, '1']\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4)]\n",
      "VERB STACK: \n",
      "\n",
      "LINE:     (VP (VBD said)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: said\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VBD\n",
      "PREV NEST: 4\n",
      "CUR NEST: 4\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "\n",
      "LINE:       (SBAR\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: SBAR\n",
      "CUR_TAG: \n",
      "PREV NEST: 4\n",
      "CUR NEST: 6\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "\n",
      "LINE:         (S\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: S\n",
      "CUR_TAG: \n",
      "PREV NEST: 6\n",
      "CUR NEST: 8\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "\n",
      "LINE:           (NP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: \n",
      "PREV NEST: 8\n",
      "CUR NEST: 10\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "\n",
      "LINE:             (NP (NNP Joseph) (NNP Y.) (NNP Yun))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: Joseph Y. Yun\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNP\n",
      "PREV NEST: 10\n",
      "CUR NEST: 12\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 12)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "\n",
      "LINE:             (, ,)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: ,\n",
      "CUR_TAG: \n",
      "PREV NEST: 12\n",
      "CUR NEST: 12\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 12)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "\n",
      "LINE:             (NP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: \n",
      "PREV NEST: 12\n",
      "CUR NEST: 12\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 12)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "\n",
      "LINE:               (NP (DT a) (JJ former) (NNP State) (NNP Department) (NN official))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: former State Department official\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NN\n",
      "PREV NEST: 12\n",
      "CUR NEST: 14\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 12), (u'former State Department official', 14)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "\n",
      "LINE:               (SBAR\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: SBAR\n",
      "CUR_TAG: \n",
      "PREV NEST: 14\n",
      "CUR NEST: 14\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 12), (u'former State Department official', 14)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "\n",
      "LINE:                 (WHNP (WP who))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: WHNP\n",
      "CUR_TAG: WP\n",
      "PREV NEST: 14\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 12), (u'former State Department official', 14)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "\n",
      "LINE:                 (S\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: S\n",
      "CUR_TAG: \n",
      "PREV NEST: 16\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 12), (u'former State Department official', 14)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "\n",
      "LINE:                   (VP (VBD negotiated)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: negotiated\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VBD\n",
      "PREV NEST: 16\n",
      "CUR NEST: 18\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 12), (u'former State Department official', 14)]\n",
      "VERB STACK: [(u'said', 4), (u'negotiated', 18)]\n",
      "\n",
      "LINE:                     (PP (IN with)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: with\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 18\n",
      "CUR NEST: 20\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 12), (u'former State Department official', 14)]\n",
      "VERB STACK: [(u'said', 4), (u'negotiated with', 18)]\n",
      "\n",
      "LINE:                       (NP (NNP North) (NNP Korea)))))))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: North Korea\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNP\n",
      "PREV NEST: 20\n",
      "CUR NEST: 22\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 12), (u'former State Department official', 14), (u'North Korea', 22)]\n",
      "VERB STACK: [(u'said', 4), (u'negotiated with', 18)]\n",
      "\n",
      "LINE:             (, ,))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: ,\n",
      "CUR_TAG: \n",
      "PREV NEST: 22\n",
      "CUR NEST: 12\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 12)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "NOUN LIST: [u'extravagant coverage', u'North Korea', u'meeting', u'North Korea', u'North Korea', u'former State Department official', u'former State Department official', u'Joseph Y. Yun']\n",
      "VERB LIST: ['-', u'of', u'negotiated with', '-']\n",
      "ENTITY LIST: ['-', u'LOCATION', '-', u'LOCATION', u'LOCATION', u'ORGANIZATION', u'ORGANIZATION', u'PERSON']\n",
      "SENTIMENT LIST: [1, '1', '1', 1]\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 12)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "\n",
      "LINE:           (VP (VBD suggested)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: suggested\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VBD\n",
      "PREV NEST: 12\n",
      "CUR NEST: 10\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 10)]\n",
      "VERB STACK: [(u'said', 4)]\n",
      "NOUN LIST: [u'extravagant coverage', u'North Korea', u'meeting', u'North Korea', u'North Korea', u'former State Department official', u'former State Department official', u'Joseph Y. Yun']\n",
      "VERB LIST: ['-', u'of', u'negotiated with', '-']\n",
      "ENTITY LIST: ['-', u'LOCATION', '-', u'LOCATION', u'LOCATION', u'ORGANIZATION', u'ORGANIZATION', u'PERSON']\n",
      "SENTIMENT LIST: [1, '1', '1', 1]\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 10)]\n",
      "VERB STACK: [(u'said', 4), (u'suggested', 10)]\n",
      "\n",
      "LINE:             (SBAR (IN that)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: SBAR\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 10\n",
      "CUR NEST: 12\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 10)]\n",
      "VERB STACK: [(u'said', 4), (u'suggested', 10)]\n",
      "\n",
      "LINE:               (S\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: S\n",
      "CUR_TAG: \n",
      "PREV NEST: 12\n",
      "CUR NEST: 14\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 10)]\n",
      "VERB STACK: [(u'said', 4), (u'suggested', 10)]\n",
      "\n",
      "LINE:                 (NP (NNP Mr.) (NNP Kim))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: Mr. Kim\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNP\n",
      "PREV NEST: 14\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 10), (u'Mr. Kim', 16)]\n",
      "VERB STACK: [(u'said', 4), (u'suggested', 10)]\n",
      "\n",
      "LINE:                 (VP (MD might)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: might\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: MD\n",
      "PREV NEST: 16\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 10), (u'Mr. Kim', 16)]\n",
      "VERB STACK: [(u'said', 4), (u'suggested', 10), (u'might', 16)]\n",
      "\n",
      "LINE:                   (VP (VB want)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: want\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VB\n",
      "PREV NEST: 16\n",
      "CUR NEST: 18\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 10), (u'Mr. Kim', 16)]\n",
      "VERB STACK: [(u'said', 4), (u'suggested', 10), (u'might want', 16)]\n",
      "\n",
      "LINE:                     (NP (DT a) (JJ different) (NN relationship))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: different relationship\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NN\n",
      "PREV NEST: 18\n",
      "CUR NEST: 20\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 10), (u'Mr. Kim', 16), (u'different relationship', 20)]\n",
      "VERB STACK: [(u'said', 4), (u'suggested', 10), (u'might want', 16)]\n",
      "\n",
      "LINE:                     (PP (IN with)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: with\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 20\n",
      "CUR NEST: 20\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 10), (u'Mr. Kim', 16), (u'different relationship', 20)]\n",
      "VERB STACK: [(u'said', 4), (u'suggested', 10), (u'might want', 16), (u'with', 20)]\n",
      "\n",
      "LINE:                       (NP (DT the) (NNP United) (NNPS States)))))))))))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: United States\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNPS\n",
      "PREV NEST: 20\n",
      "CUR NEST: 22\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4), (u'Joseph Y. Yun', 10), (u'Mr. Kim', 16), (u'different relationship', 20), (u'United States', 22)]\n",
      "VERB STACK: [(u'said', 4), (u'suggested', 10), (u'might want', 16), (u'with', 20)]\n",
      "\n",
      "LINE:     (. .)))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: .\n",
      "CUR_TAG: \n",
      "PREV NEST: 22\n",
      "CUR NEST: 4\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
      "NOUN STACK: [(u'North Korea', 4)]\n",
      "VERB STACK: \n",
      "NOUN LIST: [u'extravagant coverage', u'North Korea', u'meeting', u'North Korea', u'North Korea', u'former State Department official', u'former State Department official', u'Joseph Y. Yun', u'United States', u'different relationship', u'different relationship', u'Mr. Kim', u'Mr. Kim', u'Joseph Y. Yun', u'Joseph Y. Yun', u'North Korea']\n",
      "VERB LIST: ['-', u'of', u'negotiated with', '-', u'with', u'might want', u'suggested', u'said']\n",
      "ENTITY LIST: ['-', u'LOCATION', '-', u'LOCATION', u'LOCATION', u'ORGANIZATION', u'ORGANIZATION', u'PERSON', u'LOCATION', '-', '-', u'PERSON', u'PERSON', u'PERSON', u'PERSON', u'LOCATION']\n",
      "SENTIMENT LIST: [1, '1', '1', 1, '1', '2', '1', '1']\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'North Korea', 4)]\n",
      "VERB STACK: \n",
      "\n",
      "\n",
      "SEND OVER SOCKET\n",
      "Send Data: {'entities': ['-', u'LOCATION', '-', u'LOCATION', u'LOCATION', u'ORGANIZATION', u'ORGANIZATION', u'PERSON', u'LOCATION', '-', '-', u'PERSON', u'PERSON', u'PERSON', u'PERSON', u'LOCATION'], 'operation': 'append_graph_data', 'edges': ['-', u'of', u'negotiated with', '-', u'with', u'might want', u'suggested', u'said'], 'vertices': [u'extravagant coverage', u'North Korea', u'meeting', u'North Korea', u'North Korea', u'former State Department official', u'former State Department official', u'Joseph Y. Yun', u'United States', u'different relationship', u'different relationship', u'Mr. Kim', u'Mr. Kim', u'Joseph Y. Yun', u'Joseph Y. Yun', u'North Korea'], 'sentiment': [1, '1', '1', 1, '1', '2', '1', '1']}\n",
      "Sent Data\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings('ignore')\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
