{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ashwinramesh/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "import socket\n",
    "import json\n",
    "import nltk\n",
    "import operator\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stack:\n",
    "    def __init__(self):\n",
    "        self.__storage = []\n",
    "\n",
    "    def isEmpty(self):\n",
    "        return len(self.__storage) == 0\n",
    "\n",
    "    def push(self,p):\n",
    "        self.__storage.append(p)\n",
    "\n",
    "    def pop(self):\n",
    "        return self.__storage.pop()\n",
    "    \n",
    "    def pop_val1(self):\n",
    "        return self.__storage.pop()[0]\n",
    "    \n",
    "    def pop_val2(self):\n",
    "        return self.__storage.pop()[1]\n",
    "    \n",
    "    def top(self):\n",
    "        return self.__storage[-1]\n",
    "    \n",
    "    def top_at_pos_from_top(self, pos):\n",
    "        return self.__storage[self.size() - pos - 1]\n",
    "    \n",
    "    def top_val1(self):\n",
    "        return self.__storage[-1][0]\n",
    "    \n",
    "    def top_val1_at_pos_from_top(self, pos):\n",
    "        return self.__storage[self.size() - pos - 1][0]\n",
    "    \n",
    "    def top_val2(self):\n",
    "        return self.__storage[-1][1]\n",
    "    \n",
    "    def top_val2_at_pos_from_top(self, pos):\n",
    "        return self.__storage[self.size() - pos - 1][1]\n",
    "    \n",
    "    def combine_val1(self, cur_val):\n",
    "        top_val = self.pop()\n",
    "        self.push((top_val[0] + ' ' + cur_val, top_val[1]))\n",
    "\n",
    "    def update_val2(self, cur_val):\n",
    "        top_val = self.pop()\n",
    "        self.push((top_val[0], cur_val))\n",
    "        \n",
    "    def update_val2_at_pos_from_top(self, cur_val, pos):\n",
    "        self.__storage[self.size() - pos - 1] = (self.__storage[self.size() - pos - 1][0],cur_val) # Tuples are immutable\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.__storage)\n",
    "    \n",
    "    def prnt(self):\n",
    "        if len(self.__storage) > 0:\n",
    "            return str(self.__storage)\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "global _nlp\n",
    "global _ip\n",
    "global _port\n",
    "global _buffer_size\n",
    "global _sen_analyzer\n",
    "\n",
    "_nlp = None\n",
    "_ip = '100.81.36.227'\n",
    "_port = 7183\n",
    "_buffer_size = 1024\n",
    "_sen_analyzer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    global _nlp\n",
    "    global _sen_analyzer\n",
    "    _nlp = StanfordCoreNLP(r'stanford-corenlp-full-2017-06-09')\n",
    "    _sen_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    _nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def send_over_socket(data):\n",
    "    if len(data) == 0:\n",
    "        return\n",
    "    print \"Send Data: \" + str(data)\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client_socket.connect((_ip, _port))\n",
    "    client_socket.send(json.dumps(data))\n",
    "    print \"Sent Data\"\n",
    "    client_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    d = {'title': 'Trumpâ€™s Promises to Kim Jong-un Leave U.S. and Allies Scrambling', 'content': 'Along with 10 other nations, Canada is trying to revive the Trans-Pacific Partnership, a trade deal championed by the Obama administration and abandoned by Mr. Trump.'}\n",
    "    df = pd.DataFrame(data=d, index=[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_named_entity(phrase):\n",
    "    entity_dict = {}\n",
    "    allowed_entities = ['PERSON', 'ORGANIZATION', 'LOCATION']\n",
    "    for pair in _nlp.ner(phrase):\n",
    "        if pair[1] in allowed_entities:\n",
    "            entity_dict[allowed_entities.index(pair[1])] = pair[1]\n",
    "    if len(entity_dict) == 0:\n",
    "        return \"-\"\n",
    "    else:\n",
    "        entity_dict_sorted = sorted(entity_dict.items())\n",
    "        return entity_dict_sorted[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment(phrase):\n",
    "    sentiment_dict = {'pos': '2', 'neg': '-2', 'neu': '1'}\n",
    "    sentiment = _sen_analyzer.polarity_scores(phrase)\n",
    "    del sentiment['compound']\n",
    "    max_key = max(sentiment.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    if sentiment[max_key] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sentiment_dict[max_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# noun_st: stack to hold all the nouns; val1 refers to noun and val2 refers to the level of nesting\n",
    "# verb_st: stack to hold all the verbs; val1 refers to verb and val2 refers to the level of nesting\n",
    "# prev_added_verb: helps determine if the verbs are consecutive and can be combined\n",
    "# cur_nest: gives the level of nesting of the tags on the current line ( got using the number of tabs at the beginning of the line)\n",
    "# nouns_popped_count: Add two nouns at a time to noun list\n",
    "\n",
    "def parse_dependency_tree(tree):\n",
    "    # Define patterns for Regular expression matching\n",
    "    open_b, close_b = '()'\n",
    "    open_pattern, close_pattern = (re.escape(open_b), re.escape(close_b))\n",
    "    node_pattern = '[^\\s%s%s]+' % (open_pattern, close_pattern)\n",
    "    leaf_pattern = '[^\\s%s%s]+' % (open_pattern, close_pattern)\n",
    "    token_re = re.compile('%s\\s*(%s)?|%s|(%s)' % (\n",
    "            open_pattern, node_pattern, close_pattern, leaf_pattern))\n",
    "    \n",
    "    # Define other variables in the function whose values are constant or needs to be retained between lines of the tree\n",
    "    prev_added_verb = False\n",
    "    allowed_NP = ['NP', 'NN', 'NNP', 'NNPS', 'NNS', 'JJ', 'CD']\n",
    "    allowed_VP = ['VP', 'VB', 'VBP', 'VBZ', 'VBD', 'VBG', 'VBN', 'MD', 'TO', 'PP']\n",
    "    prev_nest,cur_nest = 0,0\n",
    "    noun_st = Stack()\n",
    "    verb_st = Stack()    \n",
    "    noun_list = []\n",
    "    verb_list = []\n",
    "    ner_list = []\n",
    "    sen_list = []\n",
    "    \n",
    "    # For every line in the tree\n",
    "    for line in tree.split('\\n'):\n",
    "        print '\\nLINE: ' + line\n",
    "        \n",
    "        #### 1. Find and process all matching patterns ####\n",
    "        \n",
    "        main_tag, cur_tag, noun, verb = '','','',''\n",
    "        prev_nest = cur_nest\n",
    "        cur_nest = len(line) - len(line.lstrip(' '))\n",
    "        \n",
    "        for match in token_re.finditer(line):\n",
    "            token = match.group()\n",
    "            # Beginning of a tree/subtree\n",
    "            if token[0] == open_b:\n",
    "                if main_tag == '':\n",
    "                    # main_tag is usually ROOT, S, NP, VP, JJ etc\n",
    "                    main_tag = token[1:].lstrip()\n",
    "                else:\n",
    "                    # get the current tag, the nested one\n",
    "                    cur_tag = token[1:].lstrip()    \n",
    "            # End of a tree/subtree - nothing to do\n",
    "            elif token == close_b:\n",
    "                ignore = 1\n",
    "            # Leaf node\n",
    "            else:\n",
    "                # if main tag is a noun (or adjective) and the nested tags are part of the noun, combine them all\n",
    "                if main_tag in allowed_NP and (cur_tag in allowed_NP or cur_tag == ''):\n",
    "                    noun += token + ' '\n",
    "                # if main tag is a verb (or preposition) and the nested tags are part of the verb, combine them all\n",
    "                elif main_tag in allowed_VP and (cur_tag in allowed_VP or cur_tag == ''):\n",
    "                    verb += token + ' '\n",
    "                # if main tag is a preposition, regardless of inner tags, consider it a verb\n",
    "                elif main_tag == 'PP':\n",
    "                    verb += token + ' '\n",
    "\n",
    "        print '#### 1. Find and process all matching patterns ####'\n",
    "        print 'NOUN: ' + noun.rstrip()\n",
    "        print 'VERB: ' + verb.rstrip()\n",
    "        print 'MAIN_TAG: ' + main_tag\n",
    "        print 'CUR_TAG: ' + cur_tag\n",
    "        print 'PREV NEST: ' + str(prev_nest)\n",
    "        print 'CUR NEST: ' + str(cur_nest)\n",
    "        \n",
    "        #### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
    "        \n",
    "        if cur_nest < prev_nest:\n",
    "            print '#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####'\n",
    "            while noun_st.size() > 0 and cur_nest <= noun_st.top_val2():\n",
    "                if noun_st.size() > 1:\n",
    "                    popped_noun = noun_st.pop()\n",
    "                    if cur_nest <= noun_st.top_val2():\n",
    "                        v1 = popped_noun[0];\n",
    "                        v2 = noun_st.top()[0];\n",
    "                        noun_list.append(v1)\n",
    "                        noun_list.append(v2)\n",
    "                        ner_list.append(get_named_entity(v1))\n",
    "                        ner_list.append(get_named_entity(v2))\n",
    "                    else:\n",
    "                        if cur_nest <= verb_st.top_val2():\n",
    "                            v1 = popped_noun[0];\n",
    "                            v2 = noun_st.top()[0];\n",
    "                            noun_list.append(v1)\n",
    "                            noun_list.append(v2)\n",
    "                            ner_list.append(get_named_entity(v1))\n",
    "                            ner_list.append(get_named_entity(v2))\n",
    "                        else:\n",
    "                            noun_st.push(popped_noun)\n",
    "                            break\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "                if verb_st.size() > 0 and (verb_st.top_val2() >= noun_st.top_val2() or verb_st.top_val2() >= popped_noun[1]):\n",
    "                    e = verb_st.pop_val1()\n",
    "                    verb_list.append(e)\n",
    "                    sen_list.append(get_sentiment(e))\n",
    "                else:\n",
    "                    verb_list.append('-') \n",
    "                    sen_list.append('0')\n",
    "                    \n",
    "            if noun_st.size() > 0:\n",
    "                noun_st.update_val2(min(cur_nest,noun_st.top_val2()))\n",
    "                    \n",
    "            cur_pos_from_top = 0\n",
    "            while verb_st.size() > cur_pos_from_top and cur_nest < verb_st.top_val2_at_pos_from_top(cur_pos_from_top):\n",
    "                verb_st.update_val2_at_pos_from_top(cur_nest, cur_pos_from_top)\n",
    "                cur_pos_from_top += 1\n",
    "\n",
    "            print 'NOUN STACK: ' + noun_st.prnt()\n",
    "            print 'VERB STACK: ' + verb_st.prnt()\n",
    "            print 'NOUN LIST: ' + str(noun_list)\n",
    "            print 'VERB LIST: ' + str(verb_list)\n",
    "            print 'ENTITY LIST: ' + str(ner_list)\n",
    "            print 'SENTIMENT LIST: ' + str(sen_list)\n",
    "                \n",
    "        #### 3. Add necessary elements to stack ####\n",
    "    \n",
    "        if noun != '':\n",
    "            noun_st.push((noun.rstrip(), cur_nest))\n",
    "            prev_added_verb = False\n",
    "            \n",
    "        if verb != '':\n",
    "            if prev_added_verb == True and verb_st.size() > 0:\n",
    "                verb_st.combine_val1(verb.rstrip())\n",
    "            else:\n",
    "                verb_st.push((verb.rstrip(), cur_nest))\n",
    "            prev_added_verb = True\n",
    "\n",
    "        print '#### 3. Add necessary elements to stack ####'\n",
    "        print 'NOUN STACK: ' + noun_st.prnt()\n",
    "        print 'VERB STACK: ' + verb_st.prnt()\n",
    "        \n",
    "    return noun_list, verb_list, ner_list, sen_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_data(data):\n",
    "    # For each article\n",
    "    for index, row in data.iterrows(): \n",
    "        # Get the content\n",
    "        content = row['content']\n",
    "\n",
    "        # Split content into sentenes\n",
    "        result = _nlp.annotate(content,\n",
    "                               properties={\n",
    "                                   'annotators': 'ssplit',\n",
    "                                   'outputFormat': 'json'\n",
    "                               })\n",
    "        annotated_content = json.loads(result)\n",
    "\n",
    "        # For each sentence\n",
    "        for annotated_sentence in annotated_content['sentences']:\n",
    "            sentence = ' '.join([t['word'] for t in annotated_sentence['tokens']])\n",
    "            \n",
    "            # Get the dependency tree for the sentence\n",
    "            tree = _nlp.parse(sentence)\n",
    "            vertices, edges, entities, sentiment = parse_dependency_tree(tree)\n",
    "            print '\\n\\nSEND OVER SOCKET'\n",
    "            socket_data = {}\n",
    "            socket_data['operation'] = 'append_graph_data'\n",
    "            socket_data['vertices'] = vertices\n",
    "            socket_data['edges'] = edges\n",
    "            socket_data['entities'] = entities\n",
    "            socket_data['sentiment'] = sentiment\n",
    "            send_over_socket(socket_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample_results():\n",
    "    socket_data = {}\n",
    "    #socket_data['operation'] = 'get_all_persons'\n",
    "    #socket_data['operation'] = 'get_all_persons_with_degree'\n",
    "    socket_data['operation'] = 'get_sentiment_around_person'\n",
    "    socket_data['person'] = 'President Trump'\n",
    "    send_over_socket(socket_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    init()\n",
    "    data = get_data()\n",
    "    parse_data(data)\n",
    "    #get_sample_results()\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LINE: (ROOT\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: ROOT\n",
      "CUR_TAG: \n",
      "PREV NEST: 0\n",
      "CUR NEST: 0\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: \n",
      "\n",
      "LINE:   (S\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: S\n",
      "CUR_TAG: \n",
      "PREV NEST: 0\n",
      "CUR NEST: 2\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: \n",
      "\n",
      "LINE:     (PP (IN Along)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: Along\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 2\n",
      "CUR NEST: 4\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: [(u'Along', 4)]\n",
      "\n",
      "LINE:       (PP (IN with)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: with\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 4\n",
      "CUR NEST: 6\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: \n",
      "VERB STACK: [(u'Along with', 4)]\n",
      "\n",
      "LINE:         (NP (CD 10) (JJ other) (NNS nations))))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: 10 other nations\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNS\n",
      "PREV NEST: 6\n",
      "CUR NEST: 8\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 8)]\n",
      "VERB STACK: [(u'Along with', 4)]\n",
      "\n",
      "LINE:     (, ,)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: ,\n",
      "CUR_TAG: \n",
      "PREV NEST: 8\n",
      "CUR NEST: 4\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
      "NOUN STACK: [(u'10 other nations', 4)]\n",
      "VERB STACK: [(u'Along with', 4)]\n",
      "NOUN LIST: []\n",
      "VERB LIST: []\n",
      "ENTITY LIST: []\n",
      "SENTIMENT LIST: []\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4)]\n",
      "VERB STACK: [(u'Along with', 4)]\n",
      "\n",
      "LINE:     (NP (NNP Canada))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: Canada\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNP\n",
      "PREV NEST: 4\n",
      "CUR NEST: 4\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4)]\n",
      "\n",
      "LINE:     (VP (VBZ is)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: is\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VBZ\n",
      "PREV NEST: 4\n",
      "CUR NEST: 4\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is', 4)]\n",
      "\n",
      "LINE:       (VP (VBG trying)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: trying\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VBG\n",
      "PREV NEST: 4\n",
      "CUR NEST: 6\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying', 4)]\n",
      "\n",
      "LINE:         (S\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: S\n",
      "CUR_TAG: \n",
      "PREV NEST: 6\n",
      "CUR NEST: 8\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying', 4)]\n",
      "\n",
      "LINE:           (VP (TO to)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: to\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: TO\n",
      "PREV NEST: 8\n",
      "CUR NEST: 10\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to', 4)]\n",
      "\n",
      "LINE:             (VP (VB revive)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: revive\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VB\n",
      "PREV NEST: 10\n",
      "CUR NEST: 12\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:               (NP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: \n",
      "PREV NEST: 12\n",
      "CUR NEST: 14\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                 (NP (DT the) (NNP Trans-Pacific) (NNP Partnership))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: Trans-Pacific Partnership\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNP\n",
      "PREV NEST: 14\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                 (, ,)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: ,\n",
      "CUR_TAG: \n",
      "PREV NEST: 16\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                 (NP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: \n",
      "PREV NEST: 16\n",
      "CUR NEST: 16\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                   (NP (DT a) (NN trade) (NN deal))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: trade deal\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NN\n",
      "PREV NEST: 16\n",
      "CUR NEST: 18\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                   (VP\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: \n",
      "PREV NEST: 18\n",
      "CUR NEST: 18\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                     (VP (VBN championed)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: championed\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VBN\n",
      "PREV NEST: 18\n",
      "CUR NEST: 20\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4), (u'championed', 20)]\n",
      "\n",
      "LINE:                       (PP (IN by)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: by\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 20\n",
      "CUR NEST: 22\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4), (u'championed by', 20)]\n",
      "\n",
      "LINE:                         (NP (DT the) (NNP Obama) (NN administration))))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: Obama administration\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NN\n",
      "PREV NEST: 22\n",
      "CUR NEST: 24\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18), (u'Obama administration', 24)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4), (u'championed by', 20)]\n",
      "\n",
      "LINE:                     (CC and)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: CC\n",
      "CUR_TAG: \n",
      "PREV NEST: 24\n",
      "CUR NEST: 20\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "NOUN LIST: [u'Obama administration', u'trade deal']\n",
      "VERB LIST: [u'championed by']\n",
      "ENTITY LIST: [u'PERSON', '-']\n",
      "SENTIMENT LIST: ['1']\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4)]\n",
      "\n",
      "LINE:                     (VP (VBN abandoned)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: abandoned\n",
      "MAIN_TAG: VP\n",
      "CUR_TAG: VBN\n",
      "PREV NEST: 20\n",
      "CUR NEST: 20\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4), (u'abandoned', 20)]\n",
      "\n",
      "LINE:                       (PP (IN by)\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: by\n",
      "MAIN_TAG: PP\n",
      "CUR_TAG: IN\n",
      "PREV NEST: 20\n",
      "CUR NEST: 22\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4), (u'abandoned by', 20)]\n",
      "\n",
      "LINE:                         (NP (NNP Mr.) (NNP Trump))))))))))))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: Mr. Trump\n",
      "VERB: \n",
      "MAIN_TAG: NP\n",
      "CUR_TAG: NNP\n",
      "PREV NEST: 22\n",
      "CUR NEST: 24\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4), (u'Canada', 4), (u'Trans-Pacific Partnership', 16), (u'trade deal', 18), (u'Mr. Trump', 24)]\n",
      "VERB STACK: [(u'Along with', 4), (u'is trying to revive', 4), (u'abandoned by', 20)]\n",
      "\n",
      "LINE:     (. .)))\n",
      "#### 1. Find and process all matching patterns ####\n",
      "NOUN: \n",
      "VERB: \n",
      "MAIN_TAG: .\n",
      "CUR_TAG: \n",
      "PREV NEST: 24\n",
      "CUR NEST: 4\n",
      "#### 2. If exiting nesting, pop the necessary stack elements and to graph data ####\n",
      "NOUN STACK: [(u'10 other nations', 4)]\n",
      "VERB STACK: \n",
      "NOUN LIST: [u'Obama administration', u'trade deal', u'Mr. Trump', u'trade deal', u'trade deal', u'Trans-Pacific Partnership', u'Trans-Pacific Partnership', u'Canada', u'Canada', u'10 other nations']\n",
      "VERB LIST: [u'championed by', u'abandoned by', '-', u'is trying to revive', u'Along with']\n",
      "ENTITY LIST: [u'PERSON', '-', u'PERSON', '-', '-', '-', '-', u'LOCATION', u'LOCATION', '-']\n",
      "SENTIMENT LIST: ['1', '-1', '0', '0', '0']\n",
      "#### 3. Add necessary elements to stack ####\n",
      "NOUN STACK: [(u'10 other nations', 4)]\n",
      "VERB STACK: \n",
      "\n",
      "\n",
      "SEND OVER SOCKET\n",
      "Send Data: {'entities': [u'PERSON', '-', u'PERSON', '-', '-', '-', '-', u'LOCATION', u'LOCATION', '-'], 'operation': 'append_graph_data', 'edges': [u'championed by', u'abandoned by', '-', u'is trying to revive', u'Along with'], 'vertices': [u'Obama administration', u'trade deal', u'Mr. Trump', u'trade deal', u'trade deal', u'Trans-Pacific Partnership', u'Trans-Pacific Partnership', u'Canada', u'Canada', u'10 other nations'], 'sentiment': ['1', '-1', '0', '0', '0']}\n",
      "Sent Data\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings('ignore')\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
